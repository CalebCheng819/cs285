all right let's talk about how we can
train
uncertainty aware neural network models
to serve
as our uncertainty aware dynamics models
for model based rl
so how can we have uncertainty aware
models
well one very simple idea is to use
the entropy of the output distribution
i'll tell you right now this is a bad
idea this does not work
but i'm going to explain it just to make
it clear why it doesn't work
so let's say that you have your neural
network dynamics model it takes in
s and a as input and it produces p
of st plus one given stat which could be
represented by a soft max distribution
if you're in the discrete action setting
or it can be represented by
a multivariate gaussian distribution in
the continuous setting so in the
multivariate gaussian case you output a
mean and a variance
in the soft mex case you just output the
logic for every
possible next state
why is this not enough
well we talked about how uh
the the problem we're having is this
uh erroneous extrapolation so
for uh the setting where we have limited
data
we might overfit and make erroneous
predictions and the particular kind of
errors that we're especially concerned
with
are ones where the optimizer can exploit
those errors
by optimizing against our model when the
optimizer
optimizes against our model what it's
really going to be doing is going to be
finding
out of distribution actions that lead to
out-of-distribution states
that then lead to more out of
distribution states
which means that our model is going to
be asked to make predictions
for states and actions that it was not
trained on
the problem is that if the model is
outputting
the uncertainty and it's trained with a
regular maximum likelihood
the uncertainty itself will also not be
accurate for our distribution inputs
so other distribution inputs will result
in erroneous predictions like an
erroneous mean
but they'll also result in an erroneous
variance for the same exact reason
and this is because the uncertainty of
the neural net output
is the wrong kind of uncertainty
so if you imagine this highly overfitted
model you could say well what variance
is this model going to be predicted
let's say that the blue the blue curve
represents the predictions from the
model
the model outputs a mean and the
variance over y at every point
well if it looks at the trading points
the trading means
are basically exactly the same as the
actual values
so the optimal variance for it to output
is actually zero
this model will be extremely confident
but of course it's completely wrong and
we'll see exactly the same thing from
deep nets we'll see very confident
predictions that are very good on the
training points
but are both incorrect and overconfident
on the test points
and this is not something special about
neural nets it's not about neural nets
being bad
at estimating uncertainty it's just
because this is the wrong kind of
uncertainty to be predicting
this measure of entropy is not trying to
predict
the uncertainty about the model
is trying to predict how noisy the
dynamics are
see there are two types of uncertainty
and there are a variety of names that
people have used for them
but we can call them aleatoric or
statistical uncertainty
which is essentially the case where you
have a function
that is itself noisy
and then we have epistemic or model
uncertainty
which happens not because the true
function itself is noisy or not
but because you don't know what the
right function is
and these are fundamentally different
kinds of uncertainty
aliatoric uncertainty doesn't go down
necessarily as you collect
more data if the true function is noisy
no matter how much
data you collect you will have high
entropy outputs just because the true
function has high entropy
like for example if you're learning the
dynamics model
for a game of chance for game where you
roll
two dice the correct
answer for the model that models the
numerical value of the sum of those two
dice is
going to be random it's never going to
become deterministic as you collect more
data
seeing the dice roll more and more
doesn't allow you
to turn that statistic that's the
casting system into the deterministic
one
that's illeatoric uncertainty that's
when the world itself is actually random
epistemic uncertainty comes from the
fact that you don't know what the model
is
so epistemic uncertainty would be like
this the setting we had
when approaching the cliff or walking
around on the top of the mountain once
you collect enough data
that uncertainty goes away but in the
limited data regime you have to maintain
that uncertainty because you don't know
what the model actually is
this is essentially a setting where the
model is certain about the data but we
are not certain about the model
and that's what we want maximum
likelihood training doesn't give you
this
so just outputting a distribution over
the next state
or a gaussian distribution over with a
mean and variance
will not get you this capability
so how can we get it well
we can try to estimate model uncertainty
and there are a number of different
techniques for doing this
so this is basically the setting where
the model is certain about the data but
we are not certain about the model
in order to not be certain about the
model we need to represent a
distribution over models
so before we have one neural net that
output a distribution
over st plus one and it has some
parameters theta
so being uncertain about the model
really means
being uncertain about theta
so usually we would estimate theta as
the arg max
of the log probability of theta given
our data set which when we're doing
maximum likelihood estimation
we take to also be the r max of the log
probability of the data given
theta and that presumes having a uniform
prior but can we instead
estimate the full distribution p theta
given d so instead of just finding the
most likely theta
what if we actually try to estimate the
full distribution theta given d
and then use that to get our uncertainty
that is the right kind of uncertainty to
get in this situation
so the entropy of this distribution will
tell us the model uncertainty and we can
average out the parameters
and get a posterior distribution over
the next state
so when we then have to predict we would
actually integrate out our parameters
so instead of taking the most likely
theta and outputting the probability of
st
plus 1 given st80 and that most likely
theta
we'll output our parameters by
integrating out theta by taking the
integral
p of s t plus 1 given sda t comma theta
times p of theta given d d theta
now of course for large high dimensional
parameter spaces of the sort that we
would have with neural nets
performing this operation exactly is
completely intractable so we have to
resort
to a variety of different approximations
and that's what we're going to talk
about in this lecture
so intuitively you could imagine this is
producing
some distribution over next states which
is going to
integrate out out all the uncertainty in
your model
so one choice that we could use is
something called a bayesian neural
network
i'm not going to go into great detail
about bayesian neural networks
in this lecture because it requires a
little bit of machinery a little bit of
variational inference machinery
uh which we're actually going to cover
next week
but i do want to explain the high level
idea behind bayesian neural nets
so in a standard neural net of the
source shown on the left
you have inputs x and outputs y
and every weight every connection
between the hidden units the inputs and
the outputs is just a number
so all the neural nets that you've
trained so far in this class basically
work on this principle
in bayesian neural networks there's a
distribution over every weight
in the most general case there's
actually a joint distribution over all
the weights
if you want to make a prediction what
you can do is you can sample from this
distribution
essentially sample a neural net from the
distribution over neural nets
and ask it for its prediction and if you
want to get a posterior distribution
over predictions if you want to sample
from the posterior distribution you
would sample a neural net and then
sample a y given that neural net and you
could repeat this process multiple times
if you want to get many samples to get a
general impression
of the true posterior distribution y
given x with theta
having been integrated out
now modeling full joint distributions
over the parameters is very difficult
because the parameters are very high
dimensional
so there are a number of common
approximations that could be made
one approximation is to estimate the
parameter posterior this p of theta
given d
as a product of independent r marginals
this basically means that every weight
is distributed randomly
but independently of all the other
weights
this is of course not a very good
approximation because in reality
the weights have very tightly
interacting uh
effects so you know if you want to vary
one weight and you vary the other one in
the opposite direction maybe your
function doesn't change very much
but if you vary them in the penalty you
could change quite a lot so using a
product of independent marginals
to estimate the parameter posterior is a
very crude approximation
but it's a very simple and tractable one
and for that reason it is used quite
often
a common choice for the independent
marginals is to represent each marginal
with a gaussian distribution and that
means that for every weight
instead of learning its numerical value
you learn its mean value
and its variance so for each weight you
have not one number but two numbers now
you have the expected weight value and
the uncertainty about the weight
and that is a very nice intuitive
interpretation because you've gone from
learning
just a single weight vector to learning
a mean weight vector and for every
dimension you have a variance
for more details about these kinds of
methods here are a few relatively simple
papers on this topic
weight uncertainty in neural networks by
blundell at all and concrete dropout by
gold at all although there are there are
many more recent
uh substantially better methods that you
could actually use if you want to do
this in practice
so bayesian neural networks are actually
a reasonable choice to get an
uncertainty aware model
to learn more about how to train them
check out these papers
or hang on until we cover the
variational inference material next week
today we're instead going to talk about
a simpler method that from my experience
actually works a little bit better in
model based reinforcement learning
and that's to use bootstrap on songs
here is the basic idea behind bootstrap
ensembles i'll present it first
intuitively
and then discuss a little bit more
mathematically what it's doing
what if instead of training one neural
network to give us the distribution over
the next state
given the current state in action we
instead
trained many different neural networks
and we somehow differ diversified them
so that each of those neural networks
learns a slightly different function
ideally they would all do similar
and accurate things on the training data
but they would all make different
mistakes
outside of the training data
if we can train this kind of ensemble of
models then we can get them to
essentially vote on what they think the
next
state will be and the dispersion in
their votes will give us an estimate
of uncertainty
mathematically this amounts to
estimating your parameter posterior p
of theta given d as a mixture of dirac
delta distributions
so you've probably learned about
mixtures of gaussians a mixture of rock
deltas
is like a mixture of gaussians only
instead of gaussians you have very
narrow spikes so each element
has no variance it's just a mixture of
delta functions
where the where each delta function is
centered at the parameter vector
for the corresponding network in the
ensemble
so intuitively you can train multiple
models and see if they agree as your
measure of uncertainty
formally you get a parameter posterior p
of theta given d
represented as this mixture of dirac
deltas which means that if you want to
integrate out your parameters
you simply average uh over your models
so you construct a mixture distribution
where each mixture element is the
prediction of the corresponding model
now very importantly in continuous state
spaces it doesn't mean that we average
together
the actual mean state we're averaging
together the probabilities
which means that each of these models if
each of these models is gaussian
and their means are in different places
our output is not one gaussian with
the average of those means it's actually
multiple different gauss's it's actually
a mixture of gaussians
so we're mixing the probabilities not
the means
so when you implement this in homework
four don't just average together the
next states
that your models predict actually uh
treat it as a mixture of gaps
okay how can we train this bootstrap
ensemble to actually get it to represent
this parameter posterior well
one mathematical tool we can use is
something called the bootstrap
the main idea in the bootstrap is that
we take our single training set
and we generate multiple independent
data sets from the single training set
to get independent models so each model
needs to be trained
on a data set that is independent from
the data set for every other model
but still comes from the same
distribution
now if we had a very large amount of
data one very simple way we could do
this
is we could take our training set and
just chop it up into n
non-overlapping pieces and train a
different model on each piece
but that's very wasteful because we're
essentially decimating our data set
and therefore we can't have too many uh
boost traps we can't have too many
models
so in the bootstrap ensemble each of
these models is called a bootstrap
so there's a cool trick that we can do
which is going to maintain our data
efficiency
and give us as many models as we want
the idea is to train each theta i on a
data set di
which is sampled with replacement for d
so if d contains n data points d i will
also contain n data points
but they will be resampled from d with
replacement
which means that for every entry in di
you
if let's say you have n data points in d
you select an integer
uniformly at random between 1 and d and
pick the corresponding element
from from d so you select a random
integer from 1 to n pick the element
from d
write that into the first entry of di
for the second entry and di pick a
random integer between 1 and d grab it
from from d
put it in entry two for entry three
random integer from one to n
take it from d put entry three and so on
and so on and so on
in expectation you get a data set that
comes from the same distribution as d
but every individual di is going to look
a little bit different
intuitively you can think of this as
putting integer counts on every data
point
and those counts can range from 0 to n
although n is very unlikely
so every model trained on every di is
going to see a slightly different
uh data set although statistically the
data sets will be similar
and it turns out this is enough to give
you a parameter posterior
so that's the theory now in the world of
deep learning
it turns out that training a bootstrap
ensemble is actually even easier
so the basic recipe i outlined on the
previous slide
essentially works
it's a fairly crude approximation
because the number of models we would
have is usually small
right so if the cost of training one
neural net is three hours
and we have to train 10 of them that
will take 30 hours of compute
now you can parallelize it but it's
still expensive so usually we'd use a
smaller number of models
typically less than 10. so our
uncertainty
will be a fairly crude approximation to
the true parameter posterior
conveniently though it appears
experimentally that
if you're training deep neural network
models resampling with replacement
is actually usually unnecessary because
just the fact that you train your model
with stochastic gradient descent
and random initialization it usually
makes the model sufficiently independent
even if they are trained on exactly the
same data set so
when implementing this in practice we
can usually actually forego the
resampling with replacement
so that makes things a little easier
it's a it's important for theoretical
results
but practically you can you can skip it