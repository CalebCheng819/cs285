# lecture13

## part1

![image-20250802165838078](./lecture13.assets/image-20250802165838078.png)

| é—®é¢˜ç±»åˆ«             | ç†è®ºå¯è§£æ€§ | ç‰¹ç‚¹                         |
| -------------------- | ---------- | ---------------------------- |
| Multi-Armed Bandits  | å®Œå…¨å¯è§£   | æ— çŠ¶æ€ã€å•æ­¥é€‰æ‹©             |
| Contextual Bandits   | éƒ¨åˆ†å¯è§£   | æœ‰çŠ¶æ€ã€å•æ­¥                 |
| å°è§„æ¨¡æœ‰é™ MDP       | å¯åˆ†æ     | æœ‰çŠ¶æ€ã€æœ‰è½¬ç§»               |
| å¤§è§„æ¨¡ MDP / Deep RL | ä¸å¯åˆ†æ   | å›¾åƒè¾“å…¥ã€è¿ç»­ç©ºé—´ç­‰å¤æ‚åœºæ™¯ |

### bandit

å¦‚ä½•å°† bandit å»ºæ¨¡ä¸º POMDPï¼Ÿ

1. **reward åˆ†å¸ƒå‡è®¾**ï¼š

   - å‡è®¾æ¯ä¸ªè‡‚ $a_i$ çš„ reward æ˜¯ä»ä¸€ä¸ªæœªçŸ¥å‚æ•°çš„åˆ†å¸ƒä¸­é‡‡æ ·å‡ºæ¥çš„ï¼š
     $$
     r(a_i) \sim p_{\theta_i}(r_i)
     $$

   - ä¸¾ä¾‹ï¼š
     $$
     p(r_i = 1) = \theta_i, \quad p(r_i = 0) = 1 - \theta_i
     $$
     ä¹Ÿå°±æ˜¯è¯´ï¼šæ¯ä¸ªè‡‚æœ‰ä¸€ä¸ª Bernoulli åˆ†å¸ƒï¼ŒæˆåŠŸæ¦‚ç‡æ˜¯ $\theta_i$ã€‚

2. **ä¸ç¡®å®šæ€§å»ºæ¨¡**ï¼š

   - æ¯ä¸ª $\theta_i$ æ¥è‡ªæŸä¸ªå…ˆéªŒåˆ†å¸ƒï¼š$\theta_i \sim p(\theta)$ï¼Œä½†æˆ‘ä»¬**ä¸çŸ¥é“**å…·ä½“å€¼ã€‚

3. **è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æŠŠæ•´ä¸ªé—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ª POMDP**ï¼š

   - çŠ¶æ€ï¼ˆéšè—çš„ï¼‰ï¼š$\mathbf{s} = [\theta_1, \theta_2, ..., \theta_n]$
   - è§‚æµ‹ï¼ˆå³ rewardï¼‰åªèƒ½éƒ¨åˆ†é€éœ²çŠ¶æ€ä¿¡æ¯ã€‚

4. **æˆ‘ä»¬çš„ belief**ï¼ˆæˆ‘ä»¬å¯¹éšè—çŠ¶æ€çš„ä¼°è®¡ï¼‰ï¼š$\hat{p}(\theta_1, ..., \theta_n)$

### regret

ç”¨ **Regretï¼ˆæ‡Šæ‚”ï¼‰** è¡¡é‡ï¼š
$$
\text{Reg}(T) = T \cdot \mathbb{E}[r(a^\star)] - \sum_{t=1}^T r(a_t)
$$

- $\mathbb{E}[r(a^\star)]$ï¼šç†æƒ³æƒ…å†µä¸‹ï¼Œæ¯ä¸€æ­¥éƒ½æ‹¿æœ€ä¼˜è‡‚çš„æœŸæœ› rewardã€‚
- $r(a_t)$ï¼šä½ å®é™…é€‰æ‹©çš„ action è·å¾—çš„ rewardã€‚
- **Regret è¶Šå°ï¼Œè¡¨ç¤ºä½ çš„ç­–ç•¥è¶Šå¿«é€¼è¿‘æœ€ä¼˜è‡‚ã€‚**

## part2

**å¤§å¤šæ•°æ¢ç´¢ç­–ç•¥éƒ½éœ€è¦æŸç§â€œä¸ç¡®å®šæ€§ä¼°è®¡â€**ï¼š

- å³ä½¿æ˜¯ç®€å•æˆ–naÃ¯veçš„æ–¹æ³•ï¼Œæ¯”å¦‚UCBä¸­åŸºäºæ¬¡æ•°è®¡ç®—çš„ç½®ä¿¡ä¸Šç•Œï¼Œä¹Ÿæ˜¯åœ¨é‡åŒ–â€œæˆ‘ä¸ç¡®å®šè¿™ä¸ªåŠ¨ä½œçš„çœŸå®å¥–åŠ±â€ã€‚

------

 **æ¢ç´¢ç­–ç•¥é€šå¸¸éšå«åœ°èµ‹äºˆâ€œæ–°ä¿¡æ¯â€æŸç§ä»·å€¼**

1. **UCB:**

   > å‡è®¾æœªçŸ¥çš„åŠ¨ä½œå¾ˆå¯èƒ½æ˜¯å¥½çš„ï¼ˆOptimismï¼‰
   >  â†’ â€œæ²¡è¯•è¿‡çš„çœ‹èµ·æ¥å°±å‰å®³â€

2. **Thompson Sampling:**

   > å‡è®¾æŠ½æ ·çš„åéªŒæ˜¯â€œçœŸç›¸â€
   >  â†’ â€œåå¤æ¨¡æ‹Ÿä¸–ç•Œå‡è®¾åæ‹©ä¼˜è¡ŒåŠ¨â€

3. **Information Gain:**

   > æ˜ç¡®è¿½æ±‚æœ€æœ‰å­¦ä¹ ä»·å€¼çš„åŠ¨ä½œ
   >  â†’ â€œå­¦åˆ°æœ€å¤šçŸ¥è¯†â€å°±æ˜¯æœ€å¥½é€‰æ‹©

### UCBï¼ˆUpper Confidence Boundï¼‰ç®—æ³•

**ä¹è§‚ä¼°è®¡ï¼ˆOptimism in the Face of Uncertaintyï¼‰**ï¼š
$$
a = \arg\max \hat{\mu}_a + C \cdot \sigma_a
$$

- $\sigma_a$ï¼šå¯¹åŠ¨ä½œ $a$ reward ä¸ç¡®å®šæ€§çš„ä¼°è®¡ï¼ˆæ¯”å¦‚æ–¹å·®æˆ–ç½®ä¿¡åŒºé—´ï¼‰ã€‚
- $C$ï¼šæ§åˆ¶æ¢ç´¢ç¨‹åº¦çš„è¶…å‚æ•°ã€‚

è¿™ç§ç­–ç•¥å°è¯•å»å¹³è¡¡ï¼š

- **æ¢ç´¢ï¼ˆExplorationï¼‰**ï¼šé€‰ä¸ç¡®å®šæ€§å¤§çš„åŠ¨ä½œï¼›
- **åˆ©ç”¨ï¼ˆExploitationï¼‰**ï¼šé€‰å½“å‰çœ‹èµ·æ¥æœ€å¥½çš„åŠ¨ä½œã€‚

**UCB1å…¬å¼ï¼ˆAuer et al., 2002ï¼‰**ï¼š
$$
a = \arg\max \hat{\mu}_a + \sqrt{\frac{2 \ln T}{N(a)}}
$$
è§£é‡Šï¼š

- $T$ï¼šå½“å‰æ—¶é—´æ­¥æ•°ã€‚
- $N(a)$ï¼šåŠ¨ä½œ $a$ è¢«é€‰è¿‡çš„æ¬¡æ•°ã€‚
- ç¬¬äºŒé¡¹æ˜¯â€œç½®ä¿¡ä¸Šç•Œâ€ï¼šè¡¨ç¤ºæˆ‘ä»¬å¯¹è¿™ä¸ªåŠ¨ä½œçœŸå®å€¼å¯èƒ½é«˜ä¼°çš„ç¨‹åº¦ã€‚

æœ€ç»ˆå¯ä»¥è¯æ˜ï¼Œè¿™ä¸ªç®—æ³•çš„ regretï¼ˆæ‡Šæ‚”å€¼ï¼‰ï¼š
$$
\text{Reg}(T) = \mathcal{O}(\log T)
$$
è¿™è¡¨ç¤ºä½ çš„ç®—æ³•è¡¨ç°å¾—**è¶Šæ¥è¶Šæ¥è¿‘æœ€ä¼˜ç­–ç•¥**ï¼Œå¹¶ä¸”æ˜¯å·²çŸ¥æ–¹æ³•ä¸­**æœ€ä¼˜é˜¶çš„ regret ä¸Šç•Œ**ã€‚

### Thompson Samplingï¼ˆPosterior Samplingï¼‰

æ¯ä¸€è½®é€‰æ‹©åŠ¨ä½œæ—¶ï¼Œåšå¦‚ä¸‹æ“ä½œï¼š

1. **é‡‡æ ·ä¸€ä¸ªå‡è®¾æ¨¡å‹**ï¼š
   $$
   \theta_1, \dots, \theta_n \sim \hat{p}(\theta_1, \dots, \theta_n)
   $$

2. **å‡è£…è¿™æ˜¯ç°å®**ï¼ˆå³è¿™äº›å‚æ•°æ˜¯â€œçœŸå®çš„â€ï¼‰

3. **é€‰æ‹©åœ¨è¿™ä¸ªå‡è®¾æ¨¡å‹ä¸‹çš„æœ€ä¼˜åŠ¨ä½œ**ï¼ˆæ¯”å¦‚æœŸæœ› reward æœ€å¤§çš„åŠ¨ä½œï¼‰

4. **æ‰§è¡ŒåŠ¨ä½œå¹¶è§‚å¯Ÿ rewardï¼Œæ›´æ–°åéªŒåˆ†å¸ƒ**ï¼ˆå³æ ¹æ®è§‚å¯Ÿæ•°æ®æ›´æ–° $\hat{p}$ï¼‰

è¿™å°±æ˜¯â€œ**æ¦‚ç‡åŒ¹é…**â€ï¼š

- ä½ é€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡ï¼Œç­‰äºè¯¥åŠ¨ä½œæ˜¯æœ€ä¼˜åŠ¨ä½œçš„æ¦‚ç‡ã€‚

å¦‚æœæ¨¡å‹ä¼°è®¡æ­£ç¡® â†’ è¾¾æˆé«˜ rewardï¼›

å¦‚æœä¼°è®¡é”™äº† â†’ è·å¾—åä¾‹ â†’ belief è¢«æ›´æ–°ï¼›

ä¸éœ€è¦ç²¾ç¡®ä¼˜åŒ– POMDPï¼Œåªéœ€åŸºäºå½“å‰ belief é‡‡å–è´ªå©ªç­–ç•¥ã€‚

### Information Gain-Based Explorationï¼ˆåŸºäºä¿¡æ¯å¢ç›Šï¼‰

ä¿¡æ¯å¢ç›Šæ˜¯æ¢ç´¢çš„åŸºæœ¬åŸç†ä¹‹ä¸€ï¼š**æˆ‘ä»¬åº”è¯¥é€‰æ‹©æœ€èƒ½å‡å°‘ä¸ç¡®å®šæ€§çš„åŠ¨ä½œ**

1. å½“å‰æˆ‘ä»¬å¯¹ $z$ ï¼ˆlatent variableï¼‰çš„è®¤è¯†ç”¨åˆ†å¸ƒ $\hat{p}(z)$ è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ç”¨**ç†µ** $\mathcal{H}(\hat{p}(z))$ è¡¡é‡ã€‚

   ç†µè¶Šé«˜ â†’ ä¸ç¡®å®šè¶Šå¤§ï¼Œç†µè¶Šä½ â†’ çŸ¥è¯†è¶Šç²¾ç¡®

2. æ‰§è¡ŒæŸä¸ªåŠ¨ä½œï¼Œå¾—åˆ°è§‚æµ‹ $y$ åï¼Œæˆ‘ä»¬æ›´æ–°äº†å¯¹ $z$ çš„ä¼°è®¡

   æ–°çš„åéªŒåˆ†å¸ƒæ˜¯ $\hat{p}(z|y)$ï¼Œå…¶ç†µæ˜¯ $\mathcal{H}(\hat{p}(z|y))$

3. æˆ‘ä»¬æƒ³çŸ¥é“è§‚å¯Ÿ $y$ ä¹‹åæˆ‘ä»¬å¯¹ $z$ çš„äº†è§£ç¨‹åº¦æé«˜äº†å¤šå°‘ï¼š
   $$
   \text{IG}(z, y) = \mathbb{E}_y \left[\mathcal{H}(\hat{p}(z)) - \mathcal{H}(\hat{p}(z|y))\right]
   $$
   è¿™ä¸ªæœŸæœ›æ˜¯å¯¹æ‰€æœ‰å¯èƒ½çš„è§‚æµ‹ $y$ æ±‚å¹³å‡ï¼Œè¡¨è¾¾æˆ‘ä»¬æ‰§è¡Œè¿™ä¸ªåŠ¨ä½œ **æœŸæœ›æå‡äº†å¤šå°‘ä¿¡æ¯é‡**ã€‚

   - å¦‚æœ IG è¶Šå¤§ï¼Œè¡¨ç¤ºè¿™ä¸ªåŠ¨ä½œå¸¦æ¥çš„ä¿¡æ¯ä»·å€¼è¶Šé«˜ â†’ å€¼å¾—å»æ¢ç´¢

ç”±äº $y$ æ˜¯æ‰§è¡ŒæŸä¸ªåŠ¨ä½œåå¾—åˆ°çš„è§‚æµ‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å†™æˆæ¡ä»¶ä¿¡æ¯å¢ç›Šï¼š
$$
\text{IG}(z, y \mid a)
$$
ä¹Ÿå°±æ˜¯ï¼š**æ‰§è¡ŒåŠ¨ä½œ $a$ ä¹‹åå¯èƒ½è·å¾—çš„å…³äº $z$ çš„ä¿¡æ¯å¢ç›Šæ˜¯å¤šå°‘**

#### IDS

æˆ‘ä»¬å¸Œæœ›åšå†³ç­–æ—¶æ—¢ï¼š

- **æ¢ç´¢**ï¼ˆè·å–æ›´å¤šä¿¡æ¯æ¥å‡å°‘æœªæ¥ä¸ç¡®å®šæ€§ï¼‰
- åˆè¦ **åˆ©ç”¨**ï¼ˆé¿å…å½“å‰é€‰é”™ suboptimal çš„åŠ¨ä½œï¼‰

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥ä¸¤ä¸ªå…³é”®é‡ï¼š

#####  ä¿¡æ¯å¢ç›Šå’Œæ¬¡ä¼˜æ€§å®šä¹‰

- **$g(a)$** æ˜¯æ‰§è¡ŒåŠ¨ä½œ $a$ åè·å¾—çš„**ä¿¡æ¯å¢ç›Šï¼ˆInformation Gainï¼‰**
  $$
  g(a) = IG(\theta_a, r_a \mid a)
  $$
  â†’ è¡¡é‡ä½ æ‰§è¡Œ $a$ åèƒ½å­¦åˆ°å¤šå°‘å…³äºè¯¥åŠ¨ä½œæœŸæœ›å›æŠ¥ $\theta_a$ çš„ä¿¡æ¯ã€‚

------

- **$\Delta(a)$** æ˜¯åŠ¨ä½œ $a$ çš„æœŸæœ›æ¬¡ä¼˜æ€§ï¼ˆExpected Suboptimalityï¼‰ï¼š
  $$
  \Delta(a) = \mathbb{E}[r(a^*) - r(a)]
  $$
  â†’ è¡¨ç¤ºä½ é¢„è®¡å®ƒæ¯”æœ€ä¼˜åŠ¨ä½œ $a^*$ å·®å¤šå°‘ã€‚

**é€‰æ‹©åŠ¨ä½œ $a$ çš„è§„åˆ™æ˜¯ï¼š**
$$
a = \arg\min_a \frac{\Delta(a)^2}{g(a)}
$$
è¿™å¯ä»¥ç†è§£ä¸ºï¼š

- å¦‚æœæŸä¸ªåŠ¨ä½œå¾ˆå¯èƒ½æ˜¯**æ¬¡ä¼˜çš„**ï¼ˆ$\Delta(a)$ å¾ˆå¤§ï¼‰â†’ ä¸è¦é€‰å®ƒï¼ˆnumerator å¤§ï¼‰ã€‚
- å¦‚æœä¸€ä¸ªåŠ¨ä½œæ‰§è¡Œå**å­¦ä¸åˆ°ä»»ä½•ä¿¡æ¯**ï¼ˆ$g(a) \to 0$ï¼‰â†’ ä¹Ÿä¸è¦é€‰å®ƒï¼ˆdenominator å°ï¼‰ã€‚

å› æ­¤ä½ é€‰çš„åŠ¨ä½œåº”è¯¥æ˜¯ï¼š

- **æœ‰å¯èƒ½æ˜¯æœ€ä¼˜çš„**
- **è€Œä¸”è¿˜èƒ½å¸®ä½ å­¦åˆ°ä¸œè¥¿**

## part3

### å°† UCB æ‹“å±•åˆ° MDPï¼šCount-Based Exploration

æˆ‘ä»¬åœ¨ **MDP çš„çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œå¯¹ä¸Šè®¡æ•°**ï¼ˆè€Œä¸æ˜¯åªåœ¨ bandit çš„ action ä¸Šï¼‰ï¼š

- $N(s)$ï¼šæŸä¸ªçŠ¶æ€ $s$ è¢«è®¿é—®çš„æ¬¡æ•°
- $N(s, a)$ï¼šçŠ¶æ€-åŠ¨ä½œå¯¹ $(s, a)$ è¢«è®¿é—®çš„æ¬¡æ•°

ç„¶åæˆ‘ä»¬å¯ä»¥æ„é€ ä¸€ä¸ª**æ¢ç´¢å¥–åŠ±ï¼ˆexploration bonusï¼‰** $\mathcal{B}(N(s))$ï¼Œéšç€è®¿é—®æ¬¡æ•°å¢åŠ è€Œå‡å°‘ã€‚

å®šä¹‰â€œå¢å¼ºç‰ˆå¥–åŠ±â€ï¼š
$$
r^+(s, a) = r(s, a) + \mathcal{B}(N(s))
$$

- å³åœ¨åŸå§‹å¥–åŠ±ä¸ŠåŠ ä¸€ä¸ªâ€œ**ä¸ç¡®å®šåº¦å¥–åŠ±**â€ï¼Œé¼“åŠ± agent å»è®¿é—®ä»æœªè®¿é—®æˆ–è®¿é—®æ¬¡æ•°å°‘çš„çŠ¶æ€ã€‚

#### é—®é¢˜

åœ¨å°å‹ç¦»æ•£ MDP ä¸­å¯ä»¥ç›´æ¥è®°å½•è®¿é—®æ¬¡æ•°ï¼Œä½†åœ¨å¤æ‚ç¯å¢ƒï¼ˆå¦‚å›¾åƒè¾“å…¥ï¼‰ä¸­ï¼š

- çŠ¶æ€ç©ºé—´ç»„åˆçˆ†ç‚¸ï¼ˆä¾‹å¦‚è’™ç‰¹ç¥–é©¬æ¸¸æˆä¸­ï¼Œéª·é«…å’Œä¸»è§’ä½ç½®å˜æ¢æ— ç©·ï¼‰ï¼›
- åœ¨è¿ç»­ç©ºé—´ï¼ˆå¦‚æœºæ¢°è‡‚ä½ç½®ï¼‰ä¸‹ï¼ŒçŠ¶æ€å‡ ä¹ä¸å¯èƒ½å®Œå…¨é‡å¤ï¼›
- å¯¼è‡´è®¿é—®æ¬¡æ•°è¿‘ä¼¼æ°¸è¿œä¸º 1ï¼Œæ— æ³•æœ‰æ•ˆä½¿ç”¨ countã€‚

> å› æ­¤éœ€è¦ä½¿ç”¨â€œçŠ¶æ€ç›¸ä¼¼æ€§â€æ¥æ³›åŒ–è®¡æ•° â†’ ä½¿ç”¨ density æ¨¡å‹ã€‚

### â€œä¼ªè®¡æ•°â€ï¼ˆpseudo-countï¼‰

- åœ¨å°å‹æœ‰é™ MDP ä¸­ï¼Œå¯ä»¥ç²¾ç¡®ç»Ÿè®¡çŠ¶æ€è®¿é—®æ¬¡æ•° `N(s)`ï¼Œè®¡ç®—ï¼š
  $$
  P(s) = \frac{N(s)}{n}
  $$
  è¡¨ç¤ºçŠ¶æ€ `s` è¢«è®¿é—®çš„é¢‘ç‡ï¼ˆæ¦‚ç‡å¯†åº¦ï¼‰

- è®¿é—®æ–°çŠ¶æ€ `s` ä¹‹åï¼Œæ›´æ–°ä¸ºï¼š
  $$
  P'(s) = \frac{N(s) + 1}{n + 1}
  $$

äºæ˜¯ï¼Œæˆ‘ä»¬æœ‰

1. **è®­ç»ƒå¯†åº¦æ¨¡å‹** $p_\theta(s)$ æ‹Ÿåˆç›®å‰çœ‹åˆ°çš„çŠ¶æ€ $\mathcal{D}$

2. **é‡åˆ°ä¸€ä¸ªæ–°çŠ¶æ€** $s_i$

3. ç”¨ $\mathcal{D} \cup \{s_i\}$ é‡æ–°è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ $p_{\theta'}(s)$

4. è®°å½•è¯¥çŠ¶æ€åœ¨ä¸¤ä¸ªæ¨¡å‹ä¸­çš„æ¦‚ç‡ï¼š
   $$
   p_\theta(s_i),\quad p_{\theta'}(s_i)
   $$

5. æ ¹æ®è¿™ä¸¤ä¸ªæ¦‚ç‡ï¼Œ**åæ¨å‡ºè¯¥çŠ¶æ€çš„ä¼ªè®¡æ•° $\hat{N}(s_i)$**ï¼Œç”¨å…¬å¼ï¼š

   ä½¿ç”¨ä¸¤ä¸ªå‡è®¾ï¼š

   - $p_\theta(s_i) = \frac{\hat{N}(s_i)}{\hat{n}}$
   - $p_{\theta'}(s_i) = \frac{\hat{N}(s_i) + 1}{\hat{n} + 1}$

   è§£è¿™ä¸¤ä¸ªæ–¹ç¨‹å¯ä»¥æ¨å‡º $\hat{N}(s_i)$ å’Œ $\hat{n}$
   $$
   \hat{N}(s_i) = \hat{n} \cdot p_\theta(s_i)
   \quad \text{å…¶ä¸­}\quad 
   \hat{n} = \frac{1 - p_{\theta'}(s_i)}{p_{\theta'}(s_i) - p_\theta(s_i)} \cdot p_\theta(s_i)
   $$

#### bonus

| æ–¹æ³•åç§°                                                     | æ¢ç´¢å¥–åŠ±å‡½æ•° $\mathcal{B}(N(s))$ | ç‰¹ç‚¹/å¤‡æ³¨                                          |
| ------------------------------------------------------------ | -------------------------------- | -------------------------------------------------- |
| **UCB** (Upper Confidence Bound)                             | $\sqrt{ \frac{2 \ln n}{N(s)} }$  | æ¥è‡ª bandit ç†è®ºï¼Œä¾èµ–æ€»æ­¥æ•° $n$ï¼Œåå‘è¿‘æœŸçŠ¶æ€æ¢ç´¢ |
| **MBIE-EB** (Strehl & Littman, 2008)                         | $\sqrt{ \frac{1}{N(s)} }$        | ä¸ UCB ç±»ä¼¼ä½†æ—  $\ln n$ï¼Œæ›´ç¨³å®š                    |
| **BEB** (Bayesian Exploration Bonus, Kolter & Ng, 2009)      | $\frac{1}{N(s)}$                 | éå¹³æ–¹æ ¹ï¼Œå¥–åŠ±è¡°å‡æ›´å¿«                             |
| **Bellemare et al. (2016)** ä½¿ç”¨çš„æ˜¯ **MBIE-EB**ï¼š$\sqrt{ \frac{1}{N(s)} }$ |                                  |                                                    |

#### çŠ¶æ€å¯†åº¦æ¨¡å‹

æˆ‘ä»¬ä¸æ˜¯ä¸ºäº†ç”Ÿæˆâ€œé€¼çœŸçš„çŠ¶æ€æ ·æœ¬â€ï¼Œè€Œæ˜¯ä¸ºäº† **ä¼°è®¡æŸä¸ªçŠ¶æ€å‡ºç°çš„æ¦‚ç‡å¯†åº¦**ï¼ˆå³â€œå®ƒæœ‰å¤šæ–°å¥‡â€ï¼‰ã€‚è¿™ä¸ GANs ç­‰ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹çš„ç›®æ ‡æ­£å¥½ç›¸åã€‚

Bellemare ç­‰äººæå‡ºçš„ â€œCTSâ€ æ¨¡å‹ï¼š

> "condition each pixel on its top-left neighborhood"

- CTSï¼ˆContext-Tree Switchingï¼‰æ¨¡å‹ç”¨äºå›¾åƒå¯†åº¦å»ºæ¨¡ã€‚
- å‡è®¾ä¸€ä¸ªå›¾åƒæ˜¯äºŒç»´æ•°ç»„ï¼Œå¯¹äºä½ç½® $(i, j)$ï¼Œå…¶åƒç´ å€¼çš„æ¦‚ç‡æ˜¯åŸºäºå…¶ **å·¦ä¸Šé‚»åŸŸåƒç´ çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ** å»ºç«‹çš„ã€‚
- å›¾å³ä¸‹è§’ç¤ºæ„å›¾ä¸­çº¢è‰²ä¸ºå½“å‰åƒç´  $x_{ij}$ï¼Œè“è‰²æ˜¯å®ƒçš„â€œæ¡ä»¶é›†â€ã€‚

å…¶ä»–å¯é€‰å¯†åº¦æ¨¡å‹

> "stochastic neural networks, compression length, EX2"

- **Stochastic neural networks**ï¼šå¸¦æœ‰å™ªå£°çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºè¾“å‡ºå¯†åº¦åˆ†å¸ƒã€‚
- **Compression length**ï¼šä¿¡æ¯å‹ç¼©è§’åº¦ï¼Œå¦‚æœçŠ¶æ€èƒ½è¢«å‹ç¼©å¾ˆå¤šè¯´æ˜â€œè§å¾—å¤šâ€ï¼Œå¯†åº¦é«˜ã€‚
- **EX2**ï¼šExplicit Exploration based on Exponential family density modelsï¼Œæ˜¯ä¸€ç§ç”¨äºç¨€æœ‰çŠ¶æ€æ£€æµ‹çš„ç­–ç•¥ã€‚

## part4

### åˆ©ç”¨å“ˆå¸Œå‡½æ•°è¿›è¡ŒçŠ¶æ€è®¡æ•°ï¼ˆcounting with hashesï¼‰

![image-20250803105926611](./lecture13.assets/image-20250803105926611.png)

ä½¿ç”¨å“ˆå¸Œå¯¹çŠ¶æ€è¿›è¡Œå‹ç¼©ï¼ˆç¦»æ•£åŒ–ï¼‰

> compress state `s` into a k-bit code via Ï†(s)ï¼Œthen count N(Ï†(s))

- å®šä¹‰ä¸€ä¸ªç¼–ç å‡½æ•° Ï†(s)ï¼šæŠŠé«˜ç»´çŠ¶æ€æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´ **k-bit è¡¨ç¤º**ã€‚
- å†å¯¹ Ï†(s) çš„è¾“å‡ºåšå“ˆå¸Œï¼Œè®°å½•æ¯ä¸ª Ï†(s) çš„è®¿é—®æ¬¡æ•°ï¼šå³ `N(Ï†(s))`ã€‚
- è¿™æ ·å¯ä»¥åœ¨ Ï†(s) çš„ç©ºé—´é‡Œâ€œè®¡æ•°â€ã€‚

**çŸ­ç é•¿åº¦** â†’ å®¹æ˜“å‘ç”Ÿå“ˆå¸Œç¢°æ’ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºï¼Œä½†å®¹æ˜“æ··æ·†ä¸åŒçŠ¶æ€ã€‚

**é•¿ç é•¿åº¦** â†’ åŒºåˆ†èƒ½åŠ›å¼ºï¼Œæ³›åŒ–å·®ï¼Œè®¡æ•°ç¨€ç–ã€‚

ç›®æ ‡æ˜¯å¸Œæœ› **ç›¸ä¼¼çŠ¶æ€å‹ç¼©åä»ç„¶æ˜ å°„åˆ°ç›¸åŒå“ˆå¸Œç **ï¼Œå®ç°â€œæ³›åŒ–çš„è®¡æ•°â€ã€‚

#### æ–¹æ³•æµç¨‹ï¼š

1. è®¾è®¡ç¼–ç å™¨ $\phi(s)$ï¼Œå°†çŠ¶æ€å‹ç¼©ä¸º $k$ ä½ç¼–ç ï¼›
2. ç¡®ä¿çŠ¶æ€æ•°é‡å¤§äº $2^k$ï¼Œå¿…å®šå­˜åœ¨å“ˆå¸Œå†²çªï¼›
3. å¯¹ hash code è€Œä¸æ˜¯åŸå§‹çŠ¶æ€è®¡æ•°ï¼›
4. hash è¶ŠçŸ­ï¼Œå†²çªè¶Šå¤šï¼Œç­‰ä»·äºâ€œç›¸ä¼¼æ€§æ›´å¹¿æ³›â€çš„æ³›åŒ–ï¼›
5. ä¸ºé¿å…éšæœº hash å‡½æ•°çš„ä¸ç¨³å®šæ€§ï¼Œä½¿ç”¨ autoencoder è®­ç»ƒç¼–ç å™¨ï¼š
   - å‹ç¼©ç“¶é¢ˆå±‚ä½œä¸º hashï¼›
   - å¯ç¡®ä¿ç›¸ä¼¼çŠ¶æ€æ‹¥æœ‰è¿‘ä¼¼ hashã€‚

ä¼˜ç‚¹ï¼š

- ç®€åŒ–çŠ¶æ€ç©ºé—´ï¼Œå…·å¤‡å±€éƒ¨æ³›åŒ–èƒ½åŠ›ï¼›
- ä¸ç»å…¸ count-based bonus ä¸€è‡´ï¼Œä½†é€‚ç”¨äºé«˜ç»´ã€‚

### EX2

æ ¸å¿ƒæ€æƒ³ï¼š**â€œæ–°é¢–æ€§ = æ˜“äºè¢«è¯†åˆ«å‡ºæ¥â€**

> A state is **novel** if it is **easy to distinguish** from all previously seen states.

 äºæ˜¯æˆ‘ä»¬ç”¨ä¸€ä¸ªåˆ†ç±»å™¨æ¥åˆ¤æ–­ï¼š

- æ¯æ¬¡è§‚æµ‹åˆ°ä¸€ä¸ªæ–°çŠ¶æ€ `s`ï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨ï¼š
  - æŠŠ `s` å½“æˆæ­£ä¾‹ï¼ˆpositiveï¼‰
  - æŠŠå†å²çŠ¶æ€é›†åˆ `ğ’Ÿ` å½“æˆè´Ÿä¾‹ï¼ˆnegativeï¼‰
- ç„¶åçœ‹è¿™ä¸ªåˆ†ç±»å™¨å¤šå®¹æ˜“åŒºåˆ†å‡º `s` æ¥ã€‚

æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªâ€œåˆ¤åˆ«æ¦‚ç‡â€å‡½æ•° $D_s(s)$ï¼Œè¡¨ç¤ºåˆ†ç±»å™¨é¢„æµ‹ `s` æ˜¯æ­£æ ·æœ¬çš„æ¦‚ç‡ã€‚

äºæ˜¯â€œå¯†åº¦â€å°±ç”¨å¦‚ä¸‹å½¢å¼è¡¨ç¤ºï¼š
$$
p_Î¸(s) = \frac{1 - D_s(s)}{D_s(s)}
$$
å«ä¹‰ï¼š

- è‹¥åˆ†ç±»å™¨è®¤ä¸º `s` æ˜¯æ­£æ ·æœ¬çš„æ¦‚ç‡ **å¾ˆé«˜**ï¼Œè¯´æ˜å®ƒ**ä¸å†å²çŠ¶æ€ä¸åŒ** â†’ æ–°é¢–åº¦é«˜ â†’ `p_Î¸(s)` å° â†’ æ›´é«˜æ¢ç´¢å¥–åŠ±ã€‚
- è‹¥åˆ†ç±»å™¨è®¤ä¸º `s` æ˜¯æ­£æ ·æœ¬çš„æ¦‚ç‡ **å¾ˆä½**ï¼Œè¯´æ˜å®ƒå’Œæ—§çŠ¶æ€åˆ†å¸ƒé‡åˆ â†’ ç†Ÿæ‚‰ â†’ `p_Î¸(s)` å¤§ â†’ ä½æ¢ç´¢å¥–åŠ±ã€‚

åœ¨çœŸå®ç¯å¢ƒä¸­ï¼ŒçŠ¶æ€ `s` å‡ ä¹ **ä»ä¸é‡å¤**ã€‚

é‚£å°±æ„å‘³ç€ï¼š
 å¦‚æœæˆ‘ä»¬ä¸ºæ¯ä¸ªçŠ¶æ€éƒ½è®­ç»ƒä¸€ä¸ªåˆ¤åˆ«å™¨ï¼Œä¼šéå¸¸ä¸ç°å® â€”â€”
 è®­ç»ƒã€å­˜å‚¨ã€æ³›åŒ–éƒ½éš¾ä»¥å®ç°ã€‚

è§£å†³æ–¹æ³•ï¼š**Amortized classifier**ï¼ˆæ‘Šé”€å¼åˆ†ç±»å™¨ï¼‰

> ä¸è¦ä¸ºæ¯ä¸ª `s` éƒ½è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ã€‚
>   è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œ `D(s, sâ)`ï¼Œè¾“å…¥æ˜¯æ–°çŠ¶æ€ `s` å’Œå†å²çŠ¶æ€ `sâ`ï¼Œè¾“å‡ºé¢„æµ‹å€¼ã€‚

è¿™ä¸ªç½‘ç»œå¯ä»¥çœ‹æˆä¸€ä¸ª**é€šç”¨æ¯”è¾ƒå™¨**ï¼Œå­¦ä¼šåˆ¤æ–­â€œä¸€ä¸ªçŠ¶æ€æ˜¯å¦ä¸ exemplar åŒºåˆ†å¼€â€ã€‚

å¦‚å›¾æ‰€ç¤ºï¼š

- ä¸¤ä¸ªè¾“å…¥é€šè¿‡ encoder
- ç¼–ç ç»“æœé€å…¥ç»Ÿä¸€ç½‘ç»œ `D_{x*}(x)`
- åˆ¤åˆ«æ˜¯å¦ç›¸åŒ

è¿™å°±æ˜¯ amortized çš„æ ¸å¿ƒæ€æƒ³ï¼š**ç”¨ç»Ÿä¸€ç½‘ç»œæ›¿ä»£å¤§é‡â€œå®šåˆ¶åˆ†ç±»å™¨â€**ã€‚

### Heuristic Bonus via Prediction Error

- æˆ‘ä»¬ä¸éœ€è¦è¿™ä¸ªå¯†åº¦æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ï¼Œä¹Ÿä¸éœ€è¦å®ƒè¾“å‡ºéå¸¸å‡†ç¡®çš„å¯†åº¦å€¼ã€‚
- æˆ‘ä»¬**åªéœ€è¦å®ƒèƒ½åŒºåˆ†â€œç†Ÿæ‚‰çš„çŠ¶æ€â€ä¸â€œæ–°é¢–çš„çŠ¶æ€â€**ï¼

> é€šè¿‡è¯¯å·®æ¥å¯å‘å¼ä¼°è®¡è®¿é—®é¢‘ç‡ï¼ˆæˆ–çŠ¶æ€çš„æ–°é¢–æ€§ï¼‰

è¿™æ˜¯ä¸€ç§ç”¨äº**æ¢ç´¢ï¼ˆexplorationï¼‰ç­–ç•¥**çš„æŠ€æœ¯ï¼Œä¸ç›´æ¥è®¡ç®—çŠ¶æ€çš„è®¿é—®æ¬¡æ•°ï¼Œè€Œæ˜¯ç”¨ä¸€ä¸ªè¯¯å·®å‡½æ•°æ¥é—´æ¥è¡¡é‡çŠ¶æ€æ˜¯å¦â€œæ–°é¢–â€ã€‚

æˆ‘ä»¬æ„å»ºä¸€ä¸ªç”¨äºåˆ¤æ–­æ–°é¢–æ€§çš„è¯¯å·®æŒ‡æ ‡ï¼š

- æœ‰ä¸€ä¸ªç›®æ ‡å‡½æ•° $f^*(s, a)$ï¼šè¡¨ç¤ºä¸€ä¸ªâ€œçœŸå®â€æˆ–â€œç›®æ ‡â€è¾“å‡ºï¼ˆæ¯”å¦‚å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ç­‰ï¼‰ã€‚

- ç»™å®šä¸€ä¸ªç»éªŒæ•°æ®ç¼“å†²åŒºï¼ˆbufferï¼‰$\mathcal{D} = \{(s_i, a_i)\}$

- æˆ‘ä»¬æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ $\hat{f}_\theta(s, a)$ï¼Œå¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œè¿‘ä¼¼ã€‚

- ç„¶åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª**è¯¯å·®é¡¹**ï¼š
  $$
  \mathcal{E}(s, a) = \|\hat{f}_\theta(s, a) - f^*(s, a)\|^2
  $$

- è¿™ä¸ªè¯¯å·®è¶Šå¤§ï¼Œè¡¨ç¤ºè¿™ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹åœ¨è®­ç»ƒæ•°æ®ä¸­è¶Šâ€œå°‘è§â€æˆ–â€œæ–°é¢–â€ï¼

![image-20250803113745964](./lecture13.assets/image-20250803113745964.png)

ä¸Šé¢çš„å›¾å±•ç¤ºäº†å¦‚ä½•ç”¨è¿™ä¸ªè¯¯å·®æ¥åˆ¤æ–­æ–°é¢–æ€§ï¼š

- ç»¿è‰²çº¿ï¼šç›®æ ‡å‡½æ•° $f^*(s, a)$
- æ©™è‰²çº¿ï¼šæ¨¡å‹é¢„æµ‹ $\hat{f}_\theta(s, a)$
- è“è‰²ç‚¹ï¼šå½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹ $(s, a)$

è§‚å¯Ÿï¼š

- è“ç‚¹å¦‚æœè½åœ¨æ¨¡å‹é¢„æµ‹å’Œç›®æ ‡ä¹‹é—´è¯¯å·®å° â†’ è¯´æ˜æ¨¡å‹å¯¹å®ƒæŒæ¡å¾—è¾ƒå¥½ â†’ **ä½æ–°é¢–æ€§**
- è“ç‚¹å¦‚æœè½åœ¨è¯¯å·®å¤§çš„åœ°æ–¹ â†’ æ¨¡å‹é¢„æµ‹ä¸å‡†ç¡® â†’ **é«˜æ–°é¢–æ€§**

#### é—®é¢˜ï¼šç”¨ä»€ä¹ˆä½œä¸º $f^\star(s, a)$ï¼Ÿ

 å¸¸è§åšæ³• 1ï¼š
$$
f^\star(s, a) = s'
$$
å³ç”¨â€œ**ä¸‹ä¸€ä¸ªçŠ¶æ€**â€ä½œä¸ºç›®æ ‡ï¼Œåš**ä¸‹ä¸€çŠ¶æ€é¢„æµ‹ï¼ˆnext state predictionï¼‰**ã€‚è¿™ç§åšæ³•ä¹Ÿè·Ÿ**ä¿¡æ¯å¢ç›Šï¼ˆinformation gainï¼‰**æœ‰å…³ï¼ˆå°†åœ¨ä¸‹ä¸€è®²è®¨è®ºï¼‰ã€‚

 æ›´ç®€å•åšæ³• 2ï¼ˆRandom Network Distillation æ ¸å¿ƒï¼‰ï¼š
$$
f^\star(s, a) = f_\phi(s, a)
$$
å…¶ä¸­ $f_\phi$ æ˜¯ä¸€ä¸ª**å›ºå®šã€éšæœºåˆå§‹åŒ–çš„ç½‘ç»œ**ï¼Œå‚æ•° $\phi$ ä¸å˜ã€‚

æˆ‘ä»¬åªè®­ç»ƒ $\hat{f}_\theta(s, a)$ å»æ‹Ÿåˆå®ƒï¼Œ**è¯¯å·®è¶Šå°è¯´æ˜æ¨¡å‹è¶Šç†Ÿæ‚‰ï¼Œè¯¯å·®å¤§è¯´æ˜è¯¥ (s, a) å¯¹ç½‘ç»œæ¥è¯´æ˜¯â€œæ–°é¢–â€çš„**ã€‚

## part5

> å½¢å¼åŒ–è§£é‡Šï¼ˆè´å¶æ–¯å…¬å¼ï¼‰ï¼š
> $$
> \underbrace{p(\theta \mid \mathcal{D})}_{\text{åéªŒ}} = \frac{\underbrace{p(\mathcal{D} \mid \theta)}_{\text{ä¼¼ç„¶}} \cdot \underbrace{p(\theta)}_{\text{å…ˆéªŒ}}}{\underbrace{p(\mathcal{D})}_{\text{è¯æ®ï¼ˆå½’ä¸€åŒ–é¡¹ï¼‰}}}
> $$
>
> #### å„ä¸ªéƒ¨åˆ†è§£é‡Šï¼š
>
> - $\theta$ï¼šæˆ‘ä»¬æƒ³è¦æ¨æ–­çš„å‚æ•°ï¼ˆä¾‹å¦‚ä¸€ä¸ªæ¨¡å‹çš„å‚æ•°ï¼‰
> - $\mathcal{D}$ï¼šè§‚æµ‹åˆ°çš„æ•°æ®
> - **å…ˆéªŒ** $p(\theta)$ï¼šåœ¨çœ‹æ•°æ®ä¹‹å‰æˆ‘ä»¬å¯¹ $\theta$ çš„ä¿¡å¿µ
> - **ä¼¼ç„¶** $p(\mathcal{D} \mid \theta)$ï¼šåœ¨å‚æ•°æ˜¯ $\theta$ æ—¶è§‚æµ‹åˆ°æ•°æ® $\mathcal{D}$ çš„å¯èƒ½æ€§
> - **åéªŒ** $p(\theta \mid \mathcal{D})$ï¼šåœ¨è§‚æµ‹äº†æ•°æ®ä¹‹åå¯¹ $\theta$ çš„æ›´æ–°ä¿¡å¿µ

åœ¨**bandité—®é¢˜ä¸­**ï¼Œæˆ‘ä»¬é‡‡æ ·çš„æ˜¯**rewardçš„åˆ†å¸ƒ**ï¼ˆpÌ‚(Î¸) æ˜¯å¯¹rewardæ¨¡å‹çš„åéªŒåˆ†å¸ƒï¼‰ã€‚

ä½†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„MDPè®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒçš„æ˜¯**Qå‡½æ•°**ï¼ˆçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°ï¼‰çš„åˆ†å¸ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š

> **MDPä¸­çš„ analog æ˜¯ Q-function!**

æˆ‘ä»¬å¯ä»¥åšè¿™æ ·ä¸€ç§ Posterior Samplingï¼š

1. **ä» Q çš„åˆ†å¸ƒ p(Q) ä¸­é‡‡æ ·ä¸€ä¸ª Q å‡½æ•°**
2. **æ ¹æ®è¿™ä¸ª Q å‡½æ•°æ‰§è¡Œä¸€ä¸ª episodeï¼ˆå®Œæ•´è½¨è¿¹ï¼‰**
3. **ç”¨æ–°æ•°æ®æ›´æ–° Q çš„åˆ†å¸ƒ p(Q)**

è¿™ä¸ªè¿‡ç¨‹æ­£æ˜¯ï¼š**Bootstrapped DQNï¼ˆOsband et al.ï¼‰** ä¸­çš„æ ¸å¿ƒæ€æƒ³ã€‚

### ç”¨ Bootstrap æ„å»º Q-function åˆ†å¸ƒ

1. **ç»™å®šä¸€ä¸ªæ•°æ®é›† `ğ’Ÿ`**
2. **é‡å¤é‡‡æ ·ï¼ˆæœ‰æ”¾å›ï¼‰N æ¬¡** â†’ å¾—åˆ° N ä¸ª bootstrapped æ•°æ®é›† `ğ’Ÿâ‚, ğ’Ÿâ‚‚, ..., ğ’Ÿâ‚™`
3. **æ¯ä¸ª bootstrapped æ•°æ®é›† `ğ’Ÿáµ¢` ç”¨æ¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹ `f_{Î¸áµ¢}`**
4. **é‡‡æ ·ä¸€ä¸ªæ¨¡å‹ `f_{Î¸áµ¢}` å°±ç­‰ä»·äºä» `p(Q)` ä¸­é‡‡æ · Q å‡½æ•°**

è¿™ä¸è´å¶æ–¯çš„æ€æƒ³ç›¸ä¼¼ï¼šæˆ‘ä»¬ä¸ç›´æ¥è¡¨ç¤ºåéªŒåˆ†å¸ƒï¼Œè€Œæ˜¯é€šè¿‡å¤šä¸ªæ¨¡å‹çš„é›†æˆæ¥éšå¼è¡¨ç¤ºå®ƒã€‚

è®­ç»ƒ N ä¸ªç¥ç»ç½‘ç»œä»£ä»·å¤ªé«˜ï¼Œèƒ½å¦é¿å…ï¼Ÿ

è§£æ³•ï¼š**å…±äº«ä¸»å¹²ç½‘ç»œ + å¤šä¸ª Head**

1. Shared Networkï¼ˆå…±äº«ä¸»å¹²ç½‘ç»œï¼‰ï¼š

- å¯¹è¾“å…¥çŠ¶æ€ $s$ï¼ˆä¾‹å¦‚å›¾åƒï¼‰è¿›è¡Œç¼–ç æˆç‰¹å¾å‘é‡ã€‚
- é€šå¸¸æ˜¯å·ç§¯å±‚ï¼ˆå›¾åƒï¼‰æˆ– MLP å±‚ï¼ˆéå›¾åƒï¼‰ã€‚
- æ‰€æœ‰ head å…±ç”¨è¿™éƒ¨åˆ†ï¼ŒèŠ‚çœè®¡ç®—ã€‚

2. å¤šä¸ª Headsï¼ˆHeadâ‚, ..., Head_Kï¼‰ï¼š

- æ¯ä¸ª head æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ Q å‡½æ•°è¿‘ä¼¼å™¨ $Q_i(s, a)$ï¼Œè´Ÿè´£æ‹Ÿåˆå„è‡ªçš„ Bootstrap æ•°æ®ã€‚
- ç»“æ„é€šå¸¸ä¸º MLPï¼Œåªæœ‰å‡ å±‚ã€‚
- å„è‡ªå¯¹åº”ä¸åŒçš„æ•°æ®é‡‡æ ·ï¼ˆå®ç° bootstrapï¼‰ã€‚

ä¼˜ç‚¹ï¼š

- ä¸éœ€ä¿®æ”¹ rewardï¼›
- æœ€ç»ˆè®­ç»ƒå‡ºçš„æ‰€æœ‰ Q-function éƒ½ä¼šæ”¶æ•›åˆ°å¥½ç­–ç•¥ï¼›
- æ— éœ€è®¾å®šæ¢ç´¢æƒé‡å‚æ•°ï¼ˆå¦‚ bonus ç³»æ•°ï¼‰ï¼›
- å®ç°ç®€å•ï¼Œä¸å½±å“åŸå§‹ç»“æ„ï¼Œå¼€é”€è¾ƒå°ã€‚

å±€é™ï¼š

- åéªŒåˆ†å¸ƒéçœŸå®è´å¶æ–¯ï¼ˆå°¤å…¶æ˜¯ multi-head å½¢å¼ï¼‰ï¼›
- å¤šæ ·æ€§æœ‰é™ï¼Œæ¢ç´¢æ·±åº¦ä»å—é™ï¼›
- åœ¨å›°éš¾æ¢ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¸å¦‚ bonus-based æ–¹æ³•ï¼›
- å®é™…ä½¿ç”¨ä¸å¦‚ count-based æˆ– curiosity æ–¹æ³•æ™®éã€‚

## part6

æ‰€æœ‰ä¿¡æ¯å¢ç›Šï¼ˆInformation Gainï¼‰æ¢ç´¢æ–¹æ³•çš„æ ¸å¿ƒç›®æ ‡ï¼š

> é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œä½¿å¾—å…¶é¢„æœŸè§‚æµ‹ç»“æœå¯¹æˆ‘ä»¬å…³å¿ƒçš„æŸä¸ªå˜é‡ $z$ çš„ä¿¡æ¯æœ€å¤šã€‚

å…³é”®é—®é¢˜ï¼šæˆ‘ä»¬æƒ³è¦äº†è§£çš„å˜é‡ $z$ æ˜¯ä»€ä¹ˆï¼Ÿ

ä¿¡æ¯å¢ç›Šå¯ä»¥å…³äºä»€ä¹ˆï¼Ÿ

1. **å…³äºå¥–åŠ±å‡½æ•° $r$**
   - ä¸æ¨èï¼šè‹¥å¥–åŠ±ç¨€ç–ï¼ˆå¦‚å¾ˆå¤š hard exploration ä»»åŠ¡ï¼‰ï¼Œå‡ ä¹æ— ä¿¡æ¯ï¼›
2. **å…³äºçŠ¶æ€åˆ†å¸ƒ $p(s)$**
   - ç¨æ˜¾å¥‡æ€ªä½†å¯ä»¥ç†è§£ï¼šå¦‚æœ agent é‡‡å–èƒ½æ˜¾è‘—æ”¹å˜ $p(s)$ çš„è¡Œä¸ºï¼Œé‚£å…¶å®æ˜¯åœ¨åšâ€œæ–°é¢–â€çš„æ¢ç´¢ï¼›
   - æœ¬è´¨ç±»ä¼¼äº count-based æˆ– density-based æ–¹æ³•ï¼›
3. **å…³äº dynamicsï¼ˆçŠ¶æ€è½¬ç§»å‡½æ•°ï¼‰ $p(s'|s,a)$**
   - å¼ºçƒˆæ¨èï¼šMDP ä¸­å”¯ä¸€å¹¿æ³›å˜åŒ–çš„æ ¸å¿ƒå› ç´ ï¼›
   - å°¤å…¶åœ¨ reward ç¨€ç–ã€åˆå§‹çŠ¶æ€å›ºå®šçš„æƒ…å½¢ä¸‹ï¼Œæ›´å€¼å¾—å­¦ä¹ çš„æ˜¯ dynamicsã€‚

> æ‰€ä»¥ï¼Œæˆ‘ä»¬èšç„¦äºå¯¹ dynamics æ¨¡å‹å‚æ•°çš„å­¦ä¹ ä½œä¸ºæ¢ç´¢çš„ç›®æ ‡ã€‚

> â€œæ— è®ºæˆ‘ä»¬è¯•å›¾ä¼°è®¡å“ªç§ä¿¡æ¯å¢ç›Šï¼Œ**ç²¾ç¡®ä½¿ç”¨å®ƒåœ¨å®è·µä¸­å‡ ä¹éƒ½æ˜¯ä¸å¯è¡Œçš„**â€

ä¸ºä»€ä¹ˆï¼Ÿ

- ä¿¡æ¯å¢ç›Šçš„å®šä¹‰æ¶‰åŠåéªŒå’Œå‰éªŒçš„ KL æ•£åº¦ï¼Œéœ€è¦è®¡ç®—å¤æ‚çš„ç§¯åˆ†ï¼š
  $$
  \text{IG} = D_{KL}\left[ p(z \mid y, a) \,\|\, p(z \mid a) \right]
  $$

- è€Œè¿™äº›åˆ†å¸ƒå¾€å¾€æ²¡æœ‰è§£æå½¢å¼ï¼Œæˆ–è€…æ— æ³•æœ‰æ•ˆé‡‡æ ·ã€‚

#### information gain (approximately)

##### Prediction Gainï¼ˆé¢„æµ‹å¢ç›Šï¼‰

$$
\text{prediction gain} = \log p_{\theta'}(s) - \log p_\theta(s)
$$

- $p_\theta(s)$ï¼šä½¿ç”¨å½“å‰å¯†åº¦ä¼°è®¡å™¨å¯¹çŠ¶æ€ $s$ çš„æ¦‚ç‡ä¼°è®¡ï¼›
- $\theta'$ï¼šè¡¨ç¤ºåœ¨çœ‹åˆ° $s$ ä¹‹åå‚æ•°æ›´æ–°è¿‡äº†ï¼›
- æ‰€ä»¥ä¸¤ä¸ª log likelihood çš„å·®å€¼è¡¡é‡çš„æ˜¯ï¼š**æ¨¡å‹å¯¹çŠ¶æ€ $s$ çš„ä¿¡å¿µå˜åŒ–äº†å¤šå°‘**ã€‚

##### Variational Inferenceï¼ˆå˜åˆ†æ¨æ–­ï¼‰

å¼•ç”¨çš„æ˜¯ VIME æ–¹æ³•ï¼ˆHouthooft et al. 2016ï¼‰

 èƒŒæ™¯ï¼š

æˆ‘ä»¬æƒ³è¦è¡¡é‡ï¼š

> **æŸä¸ªæ–°è§‚å¯Ÿåˆ°çš„ transition æ˜¯å¦æä¾›äº†å¤§é‡å…³äºç¯å¢ƒçš„çŸ¥è¯†ï¼ˆç‰¹åˆ«æ˜¯ transition dynamicsï¼‰**

 å…³é”®è¡¨è¾¾å¼ï¼š
$$
\text{IG} = D_{KL}(p(z \mid y) \,\|\, p(z))
$$
å…¶ä¸­ï¼š

- $z = \theta$ï¼šè¡¨ç¤ºç¯å¢ƒåŠ¨æ€æ¨¡å‹çš„å‚æ•°ï¼›
- $y = (s_t, a_t, s_{t+1})$ï¼šè¡¨ç¤ºä¸€ä¸ª transitionï¼›
- å³æˆ‘ä»¬è¦è¯„ä¼°ï¼šæ–°è§‚å¯Ÿ $y$ åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹å‚æ•° $\theta$ çš„åéªŒå‘ç”Ÿäº†å¤šå¤§å˜åŒ–ã€‚

------

å±•å¼€åçš„å½¢å¼ï¼š
$$
D_{KL}(p(\theta \mid h, s_t, a_t, s_{t+1}) \,\|\, p(\theta \mid h))
$$

- $h$ï¼šå†å²ä¸Šæ‰€æœ‰çš„ transitionï¼›
- åŠ ä¸Šæ–°çš„ transition $(s_t, a_t, s_{t+1})$ åï¼Œå¯¹ $\theta$ çš„ belief å˜åŒ–äº†å¤šå°‘ï¼›
- **å˜åŒ–è¶Šå¤§ â†’ ä¿¡æ¯è¶Šå¤š â†’ exploration bonus è¶Šé«˜**

------

 ç›´è§‰ï¼š

> å¦‚æœä¸€ä¸ªæ–°çš„ transition ä¼šæ˜¾è‘—æ”¹å˜æˆ‘ä»¬å¯¹ç¯å¢ƒåŠ¨åŠ›å­¦ $p(s' \mid s, a)$ çš„ä¿¡å¿µï¼Œé‚£å®ƒå°±æ˜¯ä¿¡æ¯ä¸°å¯Œçš„ â†’ å€¼å¾—æ¢ç´¢ã€‚

------

 è®¡ç®—æ–¹å¼ï¼ˆè¿‘ä¼¼ï¼‰ï¼š

- **ç›´æ¥ç®—åéªŒ KL å¤ªéš¾äº†ï¼** æ‰€ä»¥ç”¨å˜åˆ†æ¨æ–­ï¼š
  $$
  p(\theta \mid h) \approx q(\theta \mid \phi)
  $$
  ç”¨ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒ $q$ æ¥è¿‘ä¼¼çœŸå®åéªŒï¼Œç”¨ reparam trick + é‡‡æ ·ä¼°ç®— KLã€‚

ä½¿ç”¨ç‹¬ç«‹é«˜æ–¯è¿‘ä¼¼ï¼ˆmean-field assumptionï¼‰ï¼š
$$
q(\theta \mid \phi) = \prod_i \mathcal{N}(\theta_i \mid \mu_i, \sigma_i)
$$

- æ¯ä¸ªæƒé‡å‚æ•° $\theta_i$ éƒ½æœ‰è‡ªå·±çš„ $\mu_i, \sigma_i$
- æ‰€ä»¥æ•´ç½‘çš„å‚æ•°åˆ†å¸ƒå˜æˆé«˜æ–¯åˆ†å¸ƒçš„ä¹˜ç§¯

**å¦‚ä½•å­¦ä¹  $q(\theta|\phi)$ï¼Ÿ**

- ç”¨å˜åˆ†ä¸‹ç•Œï¼ˆvariational lower boundï¼‰ä¼˜åŒ–ï¼š

$$
\min D_{KL}(q(\theta \mid \phi) \,\|\, p(h|\theta)p(\theta))
$$

- è¿™æ˜¯å˜åˆ†è´å¶æ–¯ä¸­çš„ç»å…¸ç›®æ ‡ï¼Œå³è®© approximate posterior é è¿‘çœŸå®åéªŒ

**å¦‚ä½•æ ¹æ®æ–°çš„ transition æ›´æ–° beliefï¼Ÿ**

ç»™å®šä¸€ä¸ªæ–°çš„ transition $(s, a, s')$ï¼š

1. æ›´æ–° $\phi$ â†’ å¾—åˆ°æ–°çš„å‚æ•° $\phi'$
2. å³ï¼šå¯¹ç½‘ç»œå‚æ•°çš„ mean å’Œ std å†è®­ç»ƒä¸€æ­¥ï¼ˆæ¯”å¦‚æ¢¯åº¦ä¸‹é™ï¼‰
3. å¾—åˆ°æ›´æ–°åçš„åéªŒè¿‘ä¼¼åˆ†å¸ƒ $q(\theta \mid \phi')$

### other methods

#### æ–¹æ³• 1ï¼šStadie et al. 2015 â€” **Model Error as Exploration Bonus**

è¿™ä¸ªæ–¹æ³•çš„å¤§è‡´æ€è·¯å¦‚ä¸‹ï¼š

ğŸ”¹ Step 1: ä½¿ç”¨ Auto-Encoder å¯¹çŠ¶æ€å›¾åƒç¼–ç 

- åŸå§‹çŠ¶æ€å¯èƒ½æ˜¯é«˜ç»´å›¾åƒï¼›
- ç”¨ autoencoder ç¼–ç ä¸ºä½ç»´ latent å‘é‡ã€‚

ğŸ”¹ Step 2: åœ¨ latent space ä¸Šè®­ç»ƒä¸€ä¸ª forward dynamics model
$$
\hat{z}_{t+1} = f(z_t, a_t)
$$

- å…¶ä¸­ $z_t$ æ˜¯å½“å‰çŠ¶æ€ç¼–ç ï¼›
- $f$ æ˜¯ä½ è®­ç»ƒçš„é¢„æµ‹å™¨ï¼Œç”¨æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ç¼–ç ã€‚

ğŸ”¹ Step 3: ä½¿ç”¨æ¨¡å‹è¯¯å·®ä½œä¸ºæ¢ç´¢å¥–åŠ±

- å¦‚æœæ¨¡å‹åœ¨æŸä¸ª transition ä¸Šé¢„æµ‹è¯¯å·®å¤§ â‡’ æ¨¡å‹è¿˜ä¸äº†è§£è¿™é‡Œ â‡’ é«˜ä¸ç¡®å®šæ€§

- **å¥–åŠ± = prediction error**ï¼Œå¦‚ï¼š
  $$
  r_t^{\text{bonus}} = \| \hat{z}_{t+1} - z_{t+1} \|^2
  $$

- éå¸¸ç›´æ¥ã€æ˜“å®ç°ã€ä¸ä¾èµ– KL æˆ–å˜åˆ†æ¨æ–­ã€‚

#### æ–¹æ³• 2ï¼šSchmidhuber et al.ï¼ˆ1991èµ·ï¼‰â€” Intrinsic Motivation ç†è®º

æ¥è‡ª Schmidhuber çš„ç†è®ºæ¡†æ¶ï¼ˆä¹Ÿç§° formal theory of curiosity/funï¼‰ä¸­ï¼Œæå‡ºå¾ˆå¤šç§æ¿€åŠ±æ¢ç´¢çš„æ–¹å¼ï¼š

 å¸¸è§çš„ bonus ç±»å‹ï¼š

 1. **Model Error Bonus**

- å’Œ Stadie çš„æ–¹æ³•ä¸€æ ·ï¼›
- å¦‚æœæ¨¡å‹åœ¨æŸä¸ªçŠ¶æ€ä¸Šè¯¯å·®å¤§ï¼Œå°±å¥–åŠ± agent å»é‚£é‡Œã€‚

 2. **Model Gradient Bonus**

- å¦‚æœä½ è§‚å¯Ÿä¸€ä¸ªæ–°çš„ transition æ—¶ï¼Œæ¨¡å‹çš„æƒé‡æ›´æ–°å¾ˆå¤§ï¼ˆæ¢¯åº¦å¤§ï¼‰ï¼š
  $$
  \text{bonus} \propto \|\nabla_\theta \mathcal{L}_{model}\|
  $$

- è¯´æ˜æ¨¡å‹åœ¨è¿™ä¸ªæ ·æœ¬ä¸Šâ€œå­¦åˆ°äº†å¾ˆå¤šâ€ â‡’ è¿™ä¸ª transition æ˜¯æœ‰ç”¨çš„ã€‚

 3. **å…¶ä»–å˜ä½“**

- æ¯”å¦‚åŸºäºè®°å¿†è®¿é—®ã€å‹ç¼©ç‡ã€å‹ç¼©è·ç¦»ã€MDL ç­‰è¡¡é‡æ–°å¥‡æ€§æˆ–å­¦ä¹ è¿›å±•çš„æŒ‡æ ‡ã€‚

### æ¨èé˜…è¯»æ–‡çŒ®

1. Blundell et al., â€œWeight Uncertainty in Neural Networksâ€
2. Pathak et al., â€œCuriosity-Driven Exploration by Self-Supervised Predictionâ€
3. Bellemare et al., â€œUnifying Count-Based Exploration and Intrinsic Motivationâ€
4. Osband et al., â€œDeep Exploration via Bootstrapped DQNâ€
5. Fu et al., â€œEXÂ²: Exploration with Exemplar Modelsâ€
6. Tang et al., â€œ#Exploration: Structuring Exploration Using Count-Based Exploration with Hashingâ€